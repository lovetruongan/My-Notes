{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to my personal note website! This is a space where I share my thoughts, ideas, and knowledge on various topics. Feel free to explore, learn, and contribute.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Home</li> <li>Table of Contents</li> <li>Introduction</li> <li>Features</li> <li>How to Use</li> <li>Contributing</li> <li>Contact</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>This website is a collection of my personal notes and writings. It covers a wide range of topics including technology, science, personal development, and more. Whether you're here to find information, gain insights, or simply browse, I hope you find something valuable.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>User-Friendly Interface: Easy navigation and clean design to enhance your reading experience.</li> <li>Search Functionality: Quickly find notes on specific topics.</li> <li>Regular Updates: New notes and updates are added regularly.</li> <li>Community Contributions: Share your knowledge and contribute to the collection of notes.</li> </ul>"},{"location":"#how-to-use","title":"How to Use","text":"<ol> <li>Browse Topics: Use the navigation menu to browse different categories of notes.</li> <li>Search: Use the search bar to find specific notes or topics.</li> <li>Read and Learn: Click on any note to read the full content.</li> <li>Contribute: If you have valuable insights or notes to share, follow the contribution guidelines below.</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>I welcome contributions from the community! If you'd like to add your notes or improve existing content, please follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your changes.</li> <li>Make your edits or add new content.</li> <li>Submit a pull request with a description of your changes.</li> </ol> <p>Please ensure that your contributions are respectful and relevant to the topics covered on this site.</p>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions, suggestions, or feedback, feel free to reach out:</p> <ul> <li>Email: lovetruongan@gmail.com</li> </ul> <p>Thank you for visiting my note website. Happy reading and learning!</p> <p>last updated on [7/9/2024].</p>"},{"location":"AWS/AWS/","title":"Introduce","text":"<p>Amazon Web Services (AWS) is a comprehensive and widely adopted cloud platform, offering over 200 fully featured services from data centers globally. AWS provides a variety of infrastructure services such as computing power, storage options, and networking capabilities, which help businesses scale and grow. With its pay-as-you-go pricing model, AWS allows organizations to manage costs effectively while leveraging cutting-edge technologies to innovate and stay competitive.</p>"},{"location":"AWS/AWS/#key-services","title":"Key Services","text":"<ol> <li>Compute: Services like Amazon EC2 (Elastic Compute Cloud) provide scalable computing capacity.</li> <li>Storage: Solutions such as Amazon S3 (Simple Storage Service) offer scalable storage for data backup and archiving.</li> <li>Database: Managed database services like Amazon RDS (Relational Database Service) and Amazon DynamoDB.</li> <li>Networking: Services like Amazon VPC (Virtual Private Cloud) enable secure networking.</li> <li>Machine Learning: Tools like Amazon SageMaker facilitate building, training, and deploying machine learning models.</li> <li>Security: AWS provides robust security services including AWS Identity and Access Management (IAM) and AWS Shield.</li> </ol>"},{"location":"AWS/AWS/#benefits","title":"Benefits","text":"<ul> <li>Scalability: Easily scale resources up or down based on demand.</li> <li>Cost-Effectiveness: Pay only for the resources you use.</li> <li>Flexibility: Wide range of services and tools to meet diverse needs.</li> <li>Global Reach: Data centers located around the world ensure low latency and high availability.</li> <li>Security: Comprehensive security measures to protect data and applications.</li> </ul> <p>AWS continues to innovate and expand its services, making it a leading choice for businesses of all sizes looking to leverage cloud computing.</p>"},{"location":"AWS/EC2/","title":"Elastic Compute Cloud (EC2)","text":"<p>EC2 is a Regional Service and an Infrastructure as a Service (IaaS).</p> <ul> <li>A Linux-based/Windows-based/Mac-based virtual server that you can provision.</li> <li>You are limited to running On-Demand Instances per your vCPU-based On-Demand Instance limit, purchasing 20 Reserved Instances, and requesting Spot Instances per your dynamic Spot limit per region.</li> </ul>"},{"location":"AWS/EC2/#feature","title":"Feature","text":"<ul> <li>The AWS Nitro System is the underlying platform of the next generation of EC2 instances. Traditionally, hypervisors protect the physical hardware and BIOS, virtualize the CPU, storage, networking, and provide a rich set of management capabilities. With the Nitro System, these functions are offloaded to dedicated hardware and software, thereby reducing the costs of your instances in the process. Hence, the Nitro Hypervisor delivers performance that is indistinguishable from bare metal and performs better than its predecessor: the Xen Hypervisor.</li> <li>Instances: Server environments are called instances.</li> <li>Amazon Machine Images (AMIs): Package OS and additional installations in a reusable template.</li> <li>Instance Types: Various configurations of CPU, memory, storage, and networking capacity for your instances.</li> <li>General Purpose: t-type and m-type</li> <li>Compute Optimized: c-type</li> <li>Memory Optimized: r-type, x-type, and z-type</li> <li>Storage Optimized: d-type, h-type, and i-type</li> <li>Accelerated Computing: f-type, g-type, and p-type</li> <li>Key Pairs: Secure login information for your instances.</li> <li>Instance Store Volumes: Storage volumes for temporary data that are deleted when you STOP or TERMINATE your instance. Note that you can stop an EBS-backed instance but not an Instance Store-backed instance. You can only either start or terminate an Instance Store-backed instance.</li> <li>Elastic Block Store (EBS) Volumes: Persistent storage volumes for your data (see AWS storage services).</li> <li>Regions and Availability Zones: Multiple physical locations for deploying your resources, such as instances and EBS volumes (see AWS overview).</li> <li>Security Groups: A firewall that enables you to specify the protocols, ports, and source IP ranges that can reach your instances (see AWS networking and content delivery).</li> <li>Elastic IP Addresses: Static IPv4 addresses for dynamic cloud computing (see AWS networking and content delivery).</li> <li>Tags: Metadata that you can create and assign to your EC2 resources.</li> <li>Virtual Private Clouds (VPCs): Virtual networks you can create that are logically isolated from the rest of the AWS cloud, and that you can optionally connect to your own network (see AWS networking and content delivery).</li> <li>User Data: Add a script that will be run on instance boot.</li> <li>Host Recovery: Automatically restarts your instances on a new host in the event of an unexpected hardware failure on a Dedicated Host.</li> <li>EC2 Hibernation: Available for On-Demand and Reserved Instances running on freshly launched M3, M4, M5, C3, C4, C5, R3, R4, and R5 instances running Amazon Linux and Ubuntu 18.04 LTS. You can enable hibernation for your EBS-backed instances at launch. You can then hibernate and resume your instances through the AWS Management Console, or through the AWS SDK and CLI using the existing stop-instances and start-instances commands. Hibernation requires an EC2 instance to be an encrypted EBS-backed instance.</li> </ul>"},{"location":"AWS/EC2/#user-data","title":"User Data","text":"<ul> <li>Commands that run when the instance is launched for the first time (doesn't execute for subsequent runs)</li> <li>Used to automate dynamic boot tasks (that cannot be done using AMIs)</li> <li>Runs with root user privilege</li> </ul>"},{"location":"AWS/EC2/#instance-classes","title":"Instance Classes","text":"<ol> <li>General Purpose: Balance between compute, memory &amp; networking</li> <li>Compute Optimized: For compute-intensive tasks</li> <li>Memory Optimized: For in-memory databases or distributed web caches</li> <li>Storage Optimized: For storage-intensive tasks</li> </ol>"},{"location":"AWS/EC2/#security-groups","title":"Security Groups","text":"<ul> <li>Only contain Allow rules</li> <li>External firewall for EC2 instances</li> <li>Can reference a resource by IP or Security Group</li> <li>Bound to a VPC (and hence to a region)</li> <li>Blocked requests will give a Time Out error</li> </ul>"},{"location":"AWS/EC2/#iam-roles-for-ec2-instances","title":"IAM Roles for EC2 instances","text":"<p>Never enter AWS credentials into the EC2 instance, instead attach IAM Roles to the instances.</p>"},{"location":"AWS/EC2/#purchasing-options","title":"Purchasing Options","text":"<ol> <li>On-demand Instances: Pay per use, no upfront payment</li> <li>Reserved Instances: 1 or 3 year commitment</li> <li>Spot Instances: Work on a bidding basis</li> <li>Dedicated Hosts: Server hardware allocated to a specific company</li> <li>Dedicated Instances: Dedicated hardware, billed per instance</li> <li>On-Demand Capacity Reservations: Reserve capacity in an AZ</li> </ol>"},{"location":"AWS/EC2/#elastic-ip","title":"Elastic IP","text":"<ul> <li>Static Public IP that you own</li> <li>Can be attached to an EC2 instance (even when stopped)</li> <li>Soft limit of 5 elastic IPs per account</li> </ul>"},{"location":"AWS/EC2/#placement-groups-placement-strategies","title":"Placement Groups (Placement Strategies)","text":"<ol> <li>Cluster: Optimize for network</li> <li>Spread: Maximize availability</li> <li>Partition: Balance of performance and availability</li> </ol>"},{"location":"AWS/EC2/#elastic-network-interface-eni","title":"Elastic Network Interface (ENI)","text":"<ul> <li>Virtual network card that gives a private IP to an EC2 instance</li> <li>Can be detached &amp; attached across instances</li> <li>Tied to the subnet (and hence to the AZ)</li> </ul>"},{"location":"AWS/EC2/#instance-states","title":"Instance States","text":"<ul> <li>Stop: EBS root volume preserved</li> <li>Terminate: EBS root volume destroyed</li> <li>Hibernate: RAM contents saved to EBS root volume</li> </ul>"},{"location":"AWS/EC2/#ec2-nitro","title":"EC2 Nitro","text":"<p>Newer virtualization technology for EC2 instances with better networking, higher EBS IOPS, and improved security.</p>"},{"location":"AWS/EC2/#storage-options","title":"Storage Options","text":"<ol> <li>Instance Store</li> <li>Elastic Block Storage (EBS)</li> <li>Elastic File System (EFS)</li> </ol>"},{"location":"AWS/EC2/#monitoring","title":"Monitoring","text":"<p>CloudWatch is used for EC2 Monitoring</p>"},{"location":"AWS/EC2/#amazon-machine-image-ami","title":"Amazon Machine Image (AMI)","text":"<ul> <li>Pre-packaged instance image</li> <li>Bound to a region (can be copied across regions)</li> </ul>"},{"location":"AWS/EC2/#instance-metadata","title":"Instance Metadata","text":"<p>URL to fetch metadata about the instance: http://169.254.169.254/latest/meta-data</p>"},{"location":"AWS/EC2/#run-command","title":"Run Command","text":"<p>Systems Manager Run Command lets you remotely manage the configuration of your managed instances.</p>"},{"location":"AWS/EC2/#instance-tenancy","title":"Instance Tenancy","text":"<ul> <li>Default: Shared hardware</li> <li>Dedicated: Single-tenant hardware</li> <li>Host: Dedicated host</li> </ul>"},{"location":"AWS/EC2/#troubleshooting","title":"Troubleshooting","text":"<p>Common reasons for immediate instance termination:</p> <ul> <li>Reached EBS volume limit</li> <li>Corrupt EBS snapshot</li> <li>Encrypted root EBS volume without proper KMS key permissions</li> <li>Missing required part in instance store-backed AMI</li> </ul>"},{"location":"AWS/IAM/","title":"Identity &amp; Access Management (IAM)","text":"<ul> <li>The Global Service in IAM allows IAM entities like roles to be used in any region without the need for recreation.</li> <li>This means that if you create a role in one region, you can use that same role in any other region without any additional configuration.</li> <li></li> <li> <p>This feature provides flexibility and simplifies the management of IAM entities across different regions in AWS.</p> </li> <li> <p>Global Service (IAM entities like roles can be used in any region without recreation)</p> </li> </ul>"},{"location":"AWS/IAM/#users-groups","title":"Users &amp; Groups","text":"<ul> <li>Groups are collections of users and have policies attached to them</li> <li>Groups cannot be nested</li> <li>User can belong to multiple groups</li> <li>User doesn't have to belong to a group</li> <li>Root User has full access to the account</li> <li>IAM User has limited permission to the account</li> <li>You should log in as an IAM user with admin access even if you have root access. This is just to be sure that nothing goes wrong by accident.</li> <li>An IAM Group is not an identity and cannot be identified as a principal in an IAM policy</li> <li>Only users and services can assume a role (not groups)</li> <li>A new IAM user created using the AWS CLI or AWS API has no AWS credentials</li> </ul>"},{"location":"AWS/IAM/#policies","title":"Policies","text":"<ul> <li>Policies are JSON documents that outline permissions for users, groups or roles</li> <li>Two types:</li> <li>User based policies:<ul> <li>IAM policies define which API calls should be allowed for a specific user</li> </ul> </li> <li>Resource based policies:<ul> <li>Control access to an AWS resource</li> <li>Grant the specified principal permission to perform actions on the resource and define under what conditions this applies</li> </ul> </li> <li>An IAM principal can access a resource if the user policy ALLOWS it OR the resource policy ALLOWS it AND there\u2019s no explicit DENY.</li> <li>Policies assigned to a user are called inline policies</li> <li>Follow least privilege principle for IAM Policies</li> </ul>"},{"location":"AWS/IAM/#policy-structure","title":"Policy Structure","text":""},{"location":"AWS/IAM/#trust-policies","title":"Trust Policies","text":"<ul> <li>Defines which principal entities (accounts, users, roles, federated users) can assume the role</li> <li>An IAM role is both an identity and a resource that supports resource-based policies.</li> <li>You must attach both a trust policy and an identity-based policy to an IAM role.</li> <li>The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.</li> </ul>"},{"location":"AWS/IAM/#roles","title":"Roles","text":"<ul> <li>Collection of policies for AWS services</li> <li>If you are going to use an IAM Service Role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile. When you create an IAM service role for EC2, the role automatically has EC2 identified as a trusted entity.</li> </ul>"},{"location":"AWS/IAM/#protect-iam-accounts","title":"Protect IAM Accounts","text":"<ul> <li>Password Policy</li> <li>Used to enforce standards for password</li> <li>password rotation</li> <li>password reuse</li> <li>Prevents brute force attack</li> <li>Multi Factor Authentication (MFA)</li> <li>Both root user and IAM users should use MFA</li> </ul>"},{"location":"AWS/IAM/#reporting-tools","title":"Reporting Tools","text":"<ul> <li>Credentials Report</li> <li>lists all the users and the status of their credentials (MFA, password rotation, etc.)</li> <li>account level - used to audit security for all the users</li> <li>Access Advisor</li> <li>shows the service permissions granted to a user and when those services were last accessed</li> <li>user-level</li> <li>used to revise policies for a specific user</li> </ul>"},{"location":"AWS/IAM/#access-keys","title":"Access Keys","text":"<ul> <li>Need to use access keys for AWS CLI and SDK</li> <li>Don't share access keys with anyone (every user can generate their own access keys)</li> <li>Access keys are only shown once and if you lose them you need to generate a new access key</li> <li>Access Key ID ~ username</li> <li>Secret Access Key ~ password</li> <li>Long-term credentials<ul> <li>Don\u2019t update manually</li> </ul> </li> <li>IAM User don\u2019t need username and password - for CLI access key is enough</li> <li>IAM User can have up to two access keys<ul> <li>Can be created, deleted, made inactive or made active</li> </ul> </li> </ul>"},{"location":"AWS/IAM/#access-keys-consist-of-two-parts","title":"Access Keys consist of two parts","text":"<ul> <li>Both are provided when created an access key</li> <li>These are only provided once - no ability to get access to the keys again. Need to be stored safely.</li> <li>Both parts are used when accessing AWS via CLI</li> <li>Access keys need to be deleted and recreated if they are leaked</li> <li>Possible to have two sets of keys such that you can create a new one, update all applications using the keys and then delete the old set</li> </ul> <p>Access Key ID: ABABABABABABABA</p> <p>Secret Access Key: oierWRhoefWORIOF/DFLWAnljef</p>"},{"location":"AWS/IAM/#guidelines","title":"Guidelines","text":"<ul> <li>Use root account only for account setup</li> <li>1 physical user = 1 IAM user</li> <li>Enforce MFA for both root and IAM users</li> <li>Never share IAM credentials &amp; Access Keys</li> </ul>"},{"location":"AWS/IAM/#policy-simulator","title":"Policy Simulator","text":"<ul> <li>Online tool that allows us to check what API calls an IAM User, Group or Role is allowed to perform based on the permissions they have.</li> </ul>"},{"location":"AWS/IAM/#permission-boundaries","title":"Permission Boundaries","text":"<ul> <li>Set the maximum permissions an IAM entity can get</li> <li>Can be applied to users and roles (not groups)</li> <li>Used to ensure some users can\u2019t escalate their privileges (make themselves admin)</li> </ul>"},{"location":"AWS/IAM/#assume-role-vs-resource-based-policy","title":"Assume Role vs Resource-based Policy","text":"<ul> <li>When you assume an IAM Role, you give up your original permissions and take the permissions assigned to the role</li> <li>When using a resource-based policy, the principal doesn\u2019t have to give up their permissions</li> </ul>"},{"location":"Algorithm/Algorithm/","title":"Algorithm","text":"<p>An algorithm is a step-by-step procedure or a set of rules for solving a specific problem or accomplishing a specific task. It is a fundamental concept in computer science and is used to design efficient and effective solutions to various computational problems. </p>"},{"location":"Algorithm/Algorithm/#characteristics-of-an-algorithm","title":"Characteristics of an Algorithm","text":"<ul> <li> <p>Input: An algorithm takes input, which can be in the form of data, variables, or parameters.</p> </li> <li> <p>Output: An algorithm produces output, which can be a result, a solution, or a transformed input.</p> </li> <li> <p>Definiteness: An algorithm must have clear and unambiguous instructions that can be followed precisely.</p> </li> <li> <p>Finiteness: An algorithm must terminate after a finite number of steps.</p> </li> <li> <p>Feasibility: An algorithm must be practical and feasible to implement using the available resources.</p> </li> </ul> <p></p>"},{"location":"Algorithm/Algorithm/#types-of-algorithms","title":"Types of Algorithms","text":"<p>There are various types of algorithms, each designed to solve different types of problems. Some common types include:</p> <ul> <li> <p>Sorting Algorithms: These algorithms arrange a list of elements in a specific order, such as ascending or descending.</p> </li> <li> <p>Searching Algorithms: These algorithms find the location or occurrence of a specific element within a collection of elements.</p> </li> <li> <p>Graph Algorithms: These algorithms analyze and manipulate graphs, which are a collection of nodes and edges.</p> </li> <li> <p>Dynamic Programming Algorithms: These algorithms break down complex problems into simpler subproblems and solve them recursively.</p> </li> <li> <p>Greedy Algorithms: These algorithms make locally optimal choices at each step to find a global optimum.</p> </li> <li> <p>Backtracking Algorithms: These algorithms explore all possible solutions by incrementally building a solution and undoing choices when necessary.</p> </li> </ul>"},{"location":"Algorithm/Algorithm/#importance-of-algorithms","title":"Importance of Algorithms","text":"<p>Algorithms are essential in computer science and programming for the following reasons:</p> <ul> <li> <p>They provide a systematic approach to problem-solving.</p> </li> <li> <p>They help in optimizing resource usage, such as time and memory.</p> </li> <li> <p>They enable the development of efficient and scalable software applications.</p> </li> <li> <p>They are the foundation for various data structures and algorithms used in software development.</p> </li> <li> <p>They are used in various domains, including artificial intelligence, machine learning, cryptography, and more.</p> </li> </ul> <p>Remember, understanding and implementing algorithms is crucial for any programmer or software engineer to become proficient in solving complex problems efficiently.</p>"},{"location":"Algorithm/Test/test/","title":"Test","text":"<p>HOMEPAGE</p> CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"FCJ_2024/FCJ_2024/","title":"Introduce","text":""},{"location":"FCJ_2024/Pipeline/pipeline/","title":"Pipeline with AWS","text":""},{"location":"Linux/Advanced-command/","title":"Advanced Linux Concepts and Commands","text":""},{"location":"Linux/Advanced-command/#system-administration","title":"System Administration","text":""},{"location":"Linux/Advanced-command/#process-management","title":"Process Management","text":"<ul> <li><code>nice</code> and <code>renice</code>: Adjust process priority</li> <li><code>ionice</code>: Set or get process I/O scheduling class and priority</li> <li><code>chrt</code>: Manipulate real-time attributes of a process</li> <li><code>taskset</code>: Set or retrieve a process's CPU affinity</li> </ul>"},{"location":"Linux/Advanced-command/#system-monitoring","title":"System Monitoring","text":"<ul> <li><code>sar</code>: Collect, report, or save system activity information</li> <li><code>iostat</code>: Report CPU statistics and I/O statistics for devices and partitions</li> <li><code>vmstat</code>: Report virtual memory statistics</li> <li><code>netstat</code>: Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships</li> </ul>"},{"location":"Linux/Advanced-command/#advanced-networking","title":"Advanced Networking","text":"<ul> <li><code>ss</code>: Another utility to investigate sockets</li> <li><code>ip</code>: Show / manipulate routing, network devices, interfaces and tunnels</li> <li><code>tc</code>: Traffic Control in Linux, used to configure the kernel packet scheduler</li> <li><code>iptables</code>: Administration tool for IPv4/IPv6 packet filtering and NAT</li> </ul>"},{"location":"Linux/Advanced-command/#file-system-management","title":"File System Management","text":""},{"location":"Linux/Advanced-command/#advanced-file-systems","title":"Advanced File Systems","text":"<ul> <li>LVM (Logical Volume Management): Flexible disk space management</li> <li>ZFS: Advanced file system with features like snapshots, copy-on-write, and RAID-Z</li> <li>Btrfs: A copy-on-write file system for Linux</li> </ul>"},{"location":"Linux/Advanced-command/#file-system-operations","title":"File System Operations","text":"<ul> <li><code>dd</code>: Convert and copy a file, write disk headers, boot records</li> <li><code>rsync</code>: Fast, versatile file copying tool for remote and local files</li> <li><code>lsof</code>: List open files</li> <li><code>fuser</code>: Identify processes using files or sockets</li> </ul>"},{"location":"Linux/Advanced-command/#shell-scripting-and-automation","title":"Shell Scripting and Automation","text":""},{"location":"Linux/Advanced-command/#advanced-bash-features","title":"Advanced Bash Features","text":"<ul> <li>Process substitution: <code>&lt;(command)</code> and <code>&gt;(command)</code></li> <li>Parameter expansion: ${parameter:-word}, ${parameter:=word}, ${parameter:?word}, ${parameter:+word}</li> <li>Brace expansion: <code>echo {1..5}</code>, <code>echo {a..z}</code></li> </ul>"},{"location":"Linux/Advanced-command/#text-processing","title":"Text Processing","text":"<ul> <li><code>awk</code>: Pattern scanning and processing language</li> <li><code>sed</code>: Stream editor for filtering and transforming text</li> <li><code>cut</code>: Remove sections from each line of files</li> <li><code>tr</code>: Translate or delete characters</li> </ul>"},{"location":"Linux/Advanced-command/#security-and-encryption","title":"Security and Encryption","text":"<ul> <li><code>openssl</code>: Cryptography toolkit</li> <li><code>gpg</code>: GNU Privacy Guard for encryption and signing</li> <li><code>fail2ban</code>: Intrusion prevention software framework</li> <li><code>auditd</code>: System call auditing</li> </ul>"},{"location":"Linux/Advanced-command/#containerization-and-virtualization","title":"Containerization and Virtualization","text":"<ul> <li>Docker: Platform for developing, shipping, and running applications in containers</li> <li>LXC/LXD: Linux Containers</li> <li>KVM: Kernel-based Virtual Machine</li> </ul>"},{"location":"Linux/Advanced-command/#performance-tuning","title":"Performance Tuning","text":"<ul> <li><code>perf</code>: Linux profiling with performance counters</li> <li><code>strace</code>: Trace system calls and signals</li> <li><code>ltrace</code>: A library call tracer</li> </ul>"},{"location":"Linux/Advanced-command/#system-boot-and-init-systems","title":"System Boot and Init Systems","text":"<ul> <li>GRUB configuration and customization</li> <li>systemd: System and service manager</li> <li>Alternatives to systemd (e.g., SysV init, Upstart)</li> </ul>"},{"location":"Linux/Advanced-command/#kernel-management","title":"Kernel Management","text":"<ul> <li>Compiling custom kernels</li> <li>Loadable Kernel Modules</li> <li>sysctl for runtime kernel parameter tuning</li> </ul>"},{"location":"Linux/Advanced-command/#advanced-networking_1","title":"Advanced Networking","text":"<ul> <li>Bridging and bonding network interfaces</li> <li>VLANs (Virtual LANs)</li> <li>Network namespaces</li> </ul>"},{"location":"Linux/Advanced-command/#high-availability-and-clustering","title":"High Availability and Clustering","text":"<ul> <li>Pacemaker and Corosync for cluster resource management</li> <li>DRBD (Distributed Replicated Block Device) for block-level replication</li> </ul>"},{"location":"Linux/Advanced-command/#backup-and-recovery","title":"Backup and Recovery","text":"<ul> <li>Amanda: Advanced Maryland Automatic Network Disk Archiver</li> <li>Bacula: Open-source network backup solution</li> </ul> <p>Remember that mastering these advanced concepts requires hands-on practice and continuous learning. Always refer to the official documentation and man pages for the most up-to-date and detailed information.</p>"},{"location":"Linux/Linux-Cheatsheet/","title":"Linux Command Cheat Sheet","text":""},{"location":"Linux/Linux-Cheatsheet/#system-commands","title":"System Commands","text":"Command Description <code>uname -a</code> Display Linux system information <code>uname -r</code> Display kernel information <code>cat /etc/os-release</code> Display OS information like name and version <code>uptime</code> Show how long the system has been running <code>hostname</code> Display the hostname <code>hostname -I</code> Display all local IP addresses <code>last reboot</code> Show reboot history <code>date</code> Display current date and time <code>cal</code> Display calendar of current month <code>whoami</code> Display current login name <code>history</code> Show command history <code>clear</code> Clear terminal <code>shutdown -h now</code> Shutdown the system <code>reboot</code> Restart the system"},{"location":"Linux/Linux-Cheatsheet/#hardware-commands","title":"Hardware Commands","text":"Command Description <code>cat /proc/cpuinfo</code> Display CPU information <code>cat /proc/meminfo</code> Display memory information <code>free -h</code> Show free and used memory (-h for human readable, -m for MB, -g for GB) <code>lspci -tv</code> Display PCI devices <code>lsusb -tv</code> Display USB devices <code>lsblk</code> Display information about block devices <code>dmidecode</code> Display DMI/SMBIOS (hardware info) from BIOS <code>hdparm -i /dev/sda</code> Display disk sda information <code>hdparm -tT /dev/sda</code> Perform read speed test on disk sda <code>badblocks -s /dev/sda</code> Test for unreadable blocks on disk sda"},{"location":"Linux/Linux-Cheatsheet/#monitoring-commands","title":"Monitoring Commands","text":"Command Description <code>mpstat 1</code> Display processor-related statistics <code>vmstat 1</code> Display virtual memory statistics <code>iostat 1</code> Display I/O statistics <code>tail -100 /var/log/messages</code> Display last 100 system logs <code>tcpdump -i eth0</code> Capture and display all packets on interface eth0 <code>tcpdump -i eth0 'port 80'</code> Monitor all traffic on port 80 (HTTP) <code>lsof</code> List all open files on the system <code>lsof -u user</code> List files opened by user <code>watch df -h</code> Execute and \"watch\" the <code>df -h</code> command"},{"location":"Linux/Linux-Cheatsheet/#user-commands","title":"User Commands","text":"Command Description <code>id</code> Display current user and group IDs <code>last</code> Show last logged in users <code>who</code> Show who is logged into the system <code>w</code> Show who is logged in and what they're doing <code>groupadd test</code> Create a group named \"test\" <code>groupdel test</code> Delete the group named \"test\" <code>useradd -c \"This is NhaX\" -m nhax</code> Create user account \"nhax\" with comment and home directory <code>userdel nhax</code> Delete user account \"nhax\" <code>usermod -aG sales nhax</code> Add user \"nhax\" to group \"sales\" <code>usermod [option] username</code> Modify user account information (group, home dir, shell, expiry date)"},{"location":"Linux/Linux-Cheatsheet/#file-and-directory-commands","title":"File and Directory Commands","text":"Command Description <code>pwd</code> Display current working directory <code>cd</code> Change to home directory <code>cd foldername</code> Change to directory \"foldername\" <code>cd ..</code> Move up one directory level <code>ls</code> List all files and folders in current directory <code>ls -al</code> List all files and folders including hidden ones, with details <code>mkdir directory</code> Create a directory named \"directory\" <code>rm file</code> Delete file <code>rm -r directory</code> Delete directory and its contents recursively <code>rm -f file</code> Force delete file without confirmation <code>rm -rf directory</code> Force delete directory recursively <code>cp file1 file2</code> Copy file1 to file2 <code>cp -r source_directory destination</code> Copy source_directory to destination <code>mv file1 file2</code> Rename or move file1 to file2 <code>ln -s /path/to/file linkname</code> Create symbolic link to linkname <code>touch file</code> Create an empty file or update file timestamps <code>cat file</code> View file contents <code>cat file1 file2 &gt; file3</code> Combine file1 and file2 and store output in file3 <code>less file</code> Browse through a file <code>head file</code> Display first 10 lines of file <code>tail file</code> Display last 10 lines of file <code>tail -f file</code> Display last 10 lines and \"watch\" the file for changes"},{"location":"Linux/Linux-Cheatsheet/#process-commands","title":"Process Commands","text":"Command Description <code>ps</code> Display currently running processes <code>pstree</code> Display processes in a tree-like diagram <code>ps -ef</code> Display all running processes on the system <code>ps -ef \\| grep processname</code> Display process named \"processname\" <code>top</code> Display top processes <code>htop</code> Interactive process viewer (more visual than top) <code>kill pid</code> Kill process with process ID pid <code>killall processname</code> Kill all processes named \"processname\" <code>program &amp;</code> Run program in background <code>bg</code> List stopped or background jobs <code>fg</code> Bring most recent background job to foreground <code>fg n</code> Bring job n to foreground <code>lsof</code> List open files <code>pidof processname</code> Get PID of any running process"},{"location":"Linux/Linux-Cheatsheet/#file-permissions-commands","title":"File Permissions Commands","text":"Command Description <code>ls -l filename</code> Check current permissions of file <code>chmod 777 filename</code> Set full permissions (read, write, execute) for everyone <code>chmod -R 777 dirname</code> Set full permissions for directory and all subdirectories <code>chmod -x filename</code> Remove execute permission from file <code>chown username filename</code> Change file ownership <code>chown user:group filename</code> Change file owner and group ownership <code>chown -R user:group dirname</code> Change ownership of directory and all subdirectories"},{"location":"Linux/Linux-Cheatsheet/#networking-commands","title":"Networking Commands","text":"Command Description <code>ip a</code> Display network interfaces and IP addresses <code>ip addr show dev eth0</code> Show eth0 address and details <code>ip addr add IP-Address dev eth1</code> Add temporary IP address to eth1 interface <code>ethtool eth0</code> Query or control network driver and hardware settings <code>ping host</code> Send ICMP echo request to host <code>whois domain</code> Display whois information for domain <code>dig domain</code> Display DNS information for domain <code>dig -x IP_ADDRESS</code> Reverse lookup of IP_ADDRESS <code>host domain</code> Display IP address for domain <code>hostname -i</code> Display network address of hostname <code>hostname -I</code> Display all local IP addresses <code>wget http://domain.com/file</code> Download file <code>netstat -nutlp</code> Display listening tcp and udp ports and corresponding programs"},{"location":"Linux/Linux-Cheatsheet/#archive-commands-tar-files","title":"Archive Commands (TAR Files)","text":"Command Description <code>tar -cvf filename.tar filename</code> Create tar archive <code>tar -xvf filename.tar</code> Extract tar archive <code>tar -tvf filename.tar</code> List contents of tar archive <code>tar -xvf filename.tar file1.txt</code> Extract single file from tar archive <code>tar -rvf filename.tar file2.txt</code> Add file to tar archive <code>zip filename.zip filename</code> Create zip archive <code>zip filename.zip file1.txt file2.txt file3.txt</code> Create zip archive with multiple files <code>zip -u filename.zip file4.txt</code> Add file to zip archive <code>zip -d filename.zip file4.txt</code> Remove file from zip archive <code>unzip -l filename.zip</code> List contents of zip archive <code>unzip filename.zip</code> Extract zip archive <code>unzip filename.zip -d /dirname</code> Extract zip archive to specific directory"},{"location":"Linux/Linux-Cheatsheet/#installing-packages","title":"Installing Packages","text":"Command Description <code>apt-get install packagename</code> Install package on Debian-based distributions <code>apt-get remove packagename</code> Remove package on Debian-based distributions <code>dpkg -l \\| grep -i installed</code> Get list of all installed packages on Debian-based distribution <code>dpkg -i packagename.deb</code> Install .deb package <code>apt-get update</code> Update repository on Debian-based distributions <code>apt-get upgrade packagename</code> Upgrade specific package on Debian-based distributions <code>apt-get autoremove</code> Remove all unwanted packages on Debian-based distributions <code>yum install packagename</code> Install package on RPM-based distributions <code>yum remove packagename</code> Remove package on RPM-based distributions <code>yum update</code> Update all system packages to latest version on RPM-based distributions <code>yum list --installed</code> List all installed packages on RPM-based distributions <code>yum list --available</code> List all available packages on RPM-based distributions"},{"location":"Linux/Linux-Cheatsheet/#search-commands","title":"Search Commands","text":"Command Description <code>grep pattern file</code> Search for \"pattern\" in file <code>grep -r pattern directory</code> Search recursively for \"pattern\" in directory <code>locate name</code> Find files and directories by name <code>find /home/john -name 'prefix*'</code> Find files in /home/john starting with \"prefix\" <code>find /home -size +100M</code> Find files larger than 100MB in /home"},{"location":"Linux/Linux-Cheatsheet/#ssh-logins","title":"SSH Logins","text":"Command Description <code>ssh host</code> Connect to host as your local username <code>ssh user@host</code> Connect to host as user <code>ssh -p port user@host</code> Connect to host using port"},{"location":"Linux/Linux-Cheatsheet/#file-transfers","title":"File Transfers","text":"Command Description <code>scp file.txt server:/tmp</code> Securely copy file.txt to the /tmp directory on the server <code>scp server:/var/www/*.html /tmp</code> Copy *.html files from server to local /tmp directory <code>scp -r server:/var/www /tmp</code> Recursively copy all files and directories from server to local /tmp directory <code>rsync -a /home /backups/</code> Synchronize /home to /backups/home <code>rsync -avz /home server:/backups/</code> Synchronize files/directories between local and remote system with compression"},{"location":"Linux/Linux-Cheatsheet/#disk-usage","title":"Disk Usage","text":"Command Description <code>df -h</code> Display free and used space on mounted filesystems <code>df -i</code> Display free and used inodes on mounted filesystems <code>fdisk -l</code> Display disk partitions, sizes, and types <code>du -ah</code> Display disk usage for all files and directories in human-readable format <code>du -sh</code> Display total disk usage of current directory"},{"location":"Linux/Linux-Cheatsheet/#security-commands","title":"Security Commands","text":"Command Description <code>passwd</code> Change password of current user <code>sudo -i</code> Switch to root account with root's environment <code>sudo -s</code> Execute your current shell with root privileges <code>sudo -l</code> List sudo privileges for current user <code>visudo</code> Edit the sudoers configuration file <code>getenforce</code> Display current SELinux mode <code>sestatus</code> Display SELinux status and configuration <code>setenforce 0</code> Change current SELinux mode to Permissive (does not persist on reboot) <code>setenforce 1</code> Change current SELinux mode to Enforcing (does not persist on reboot) <code>SELINUX=enforcing</code> Set SELinux mode to enforcing on boot in /etc/selinux/config <code>SELINUX=permissive</code> Set SELinux mode to permissive on boot in /etc/selinux/config <code>SELINUX=disabled</code> Set SELinux mode to disabled on boot in /etc/selinux/config"},{"location":"Linux/Linux/","title":"Introduction to Linux","text":""},{"location":"Linux/Linux/#what-is-linux","title":"What is Linux?","text":"<p>Linux is an open-source operating system that was created by Linus Torvalds in 1991. It is based on the Unix operating system and is known for its stability, security, and flexibility. Unlike other operating systems, Linux is free to use and modify, making it a popular choice for developers, businesses, and individuals.</p>"},{"location":"Linux/Linux/#key-features-of-linux","title":"Key Features of Linux","text":"<ul> <li>Open Source: The source code of Linux is available for anyone to view, modify, and distribute. This fosters a community-driven development process.</li> <li>Security: Linux is known for its robust security features. It is less susceptible to malware and viruses compared to other operating systems.</li> <li>Stability: Linux systems are highly stable and can run for long periods without requiring a reboot.</li> <li>Customizability: Users can customize their Linux experience extensively, from the desktop environment to the kernel itself.</li> <li>Multitasking: Linux can handle multiple tasks simultaneously without performance degradation.</li> <li>Compatibility: Linux supports a wide range of hardware and software, making it versatile for different use cases.</li> </ul>"},{"location":"Linux/Linux/#linux-distributions","title":"Linux Distributions","text":"<p>A Linux distribution (or distro) is a version of Linux that includes the Linux kernel, system libraries, and software applications. Some popular Linux distributions include:</p> <ul> <li>Ubuntu: Known for its user-friendly interface and strong community support.</li> <li>Fedora: A cutting-edge distribution that showcases the latest features and technologies.</li> <li>Debian: Known for its stability and extensive software repository.</li> <li>CentOS: A free, community-supported version of Red Hat Enterprise Linux (RHEL).</li> <li>Arch Linux: A lightweight and flexible distribution aimed at experienced users.</li> </ul>"},{"location":"Linux/Linux/#basic-linux-commands","title":"Basic Linux Commands","text":"<p>Here are some fundamental Linux commands to get you started:</p> <ul> <li><code>ls</code>: Lists the files and directories in the current directory.</li> <li><code>cd</code>: Changes the current directory.</li> <li><code>pwd</code>: Prints the current working directory.</li> <li><code>cp</code>: Copies files or directories.</li> <li><code>mv</code>: Moves or renames files or directories.</li> <li><code>rm</code>: Removes files or directories.</li> <li><code>touch</code>: Creates an empty file.</li> <li><code>mkdir</code>: Creates a new directory.</li> <li><code>chmod</code>: Changes the permissions of files or directories.</li> <li><code>chown</code>: Changes the ownership of files or directories.</li> </ul>"},{"location":"Linux/Linux/#learning-resources","title":"Learning Resources","text":"<p>There are many resources available for learning Linux, including:</p> <ul> <li>Books: \"The Linux Command Line\" by William Shotts, \"Linux for Beginners\" by Jason Cannon.</li> <li>Online Courses: Courses on platforms like Coursera, Udemy, and edX.</li> <li>Documentation: The official documentation of various Linux distributions.</li> <li>Community Forums: Websites like Stack Overflow, Reddit, and Linux-specific forums.</li> </ul>"},{"location":"Linux/Linux/#conclusion","title":"Conclusion","text":"<p>Linux is a powerful and versatile operating system that is used in a variety of environments, from servers and supercomputers to desktops and mobile devices. Its open-source nature, coupled with its stability and security, makes it an excellent choice for both beginners and experienced users. By learning Linux, you gain valuable skills that are highly sought after in the tech industry.</p>"},{"location":"Terraform/Terraform/","title":"Terraform","text":"<p>What is Terraform? Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing, popular service providers and custom in-house solutions.</p> <p>Configuration files describe to Terraform the components needed to run a single application or your entire data center. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform can determine what changed and create incremental execution plans that can be applied.</p> <p>Key features Infrastructure as code Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your data center to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used.</p> <p>Execution plans Terraform has a planning step in which it generates an execution plan. The execution plan shows what Terraform will do when you execute the apply command. This lets you avoid any surprises when Terraform manipulates infrastructure.</p> <p>Resource graph Terraform builds a graph of all your resources and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure.</p> <p>Change automation Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, which helps you avoid many possible human errors.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/","title":"Hello Node Kubernetes in GKE","text":""},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/#introduce","title":"Introduce","text":"<p>Kubernetes is an open source project (available on kubernetes.io) which can run on many different environments, from laptops to high-availability multi-node clusters; from public clouds to on-premise deployments; from virtual machines to bare metal.</p>"},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/#what-youll-learn","title":"What you'll learn","text":"<ul> <li>Create a Node.js server.</li> <li>Create a Docker container image.</li> <li>Create a container cluster.</li> <li>Create a Kubernetes pod.</li> <li>Scale up your services.</li> </ul>"},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/#task-1-create-app","title":"Task 1 : Create app","text":"<p>Create server.js <pre><code>var http = require('http');\nvar handleRequest = function(request, response) {\n  response.writeHead(200);\n  response.end(\"Hello World!\");\n}\nvar www = http.createServer(handleRequest);\nwww.listen(8080);\n</code></pre> Create Dockerfile <pre><code>FROM node:6.9.2\nEXPOSE 8080\nCOPY server.js .\nCMD node server.js \n</code></pre> Buils and run on Docker docker build -t gcr.io/PROJECT_ID/hello-node:v1 . docker run -d -p 8080:8080 gcr.io/PROJECT_ID/hello-node:v1</p> <p>Auth and push image to GCR</p>"},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/#task-2-create-cluster","title":"Task 2 : Create cluster","text":"<p>Create cluster in cloudshell <pre><code>gcloud container clusters create hello-world \\\n                --num-nodes 2 \\\n                --machine-type e2-medium \\\n                --zone \"europe-west1-b\"\n</code></pre></p>"},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/#task-3-create-deployment","title":"Task 3 : Create deployment","text":"<p><pre><code>kubectl create deployment hello-node \\\n    --image=gcr.io/qwiklabs-gcp-01-e142374127cb/hello-node:v1\n</code></pre> Create deployment with Hello image in GCR</p>"},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/#task-4-allow-external-traffic","title":"Task 4 : Allow external traffic","text":"<p><pre><code>kubectl expose deployment hello-node --type=\"LoadBalancer\" --port=8080\n</code></pre> My application will be expose on the internet on port 8080 with external ip</p>"},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/#task-5-scale-up-service","title":"Task 5 : Scale up service","text":"<pre><code>kubectl scale deployment hello-node --replicas=4\n</code></pre>"},{"location":"blog/2024/08/03/hello-node-kubernetes-in-gke/#task-6-roll-back","title":"Task 6 : Roll back","text":"<p>Edit the server.js file <pre><code>var http = require('http');\nvar handleRequest = function(request, response) {\n  response.writeHead(200);\n  response.end(\"Hello Nguyen Bui Truong An\");\n}\nvar www = http.createServer(handleRequest);\nwww.listen(8080);\n</code></pre> Build and push Hello/v2 into GCR <pre><code>kubectl edit deployment hello-node\n</code></pre> Change image version to Hello/v2 and new version would be apply </p>"},{"location":"blog/2024/07/03/terraform-basic-lab-01/","title":"Terraform basic lab 01","text":"<p>touch instance.tf</p> <p>resource \"google_compute_instance\" \"terraform\" {   project      = \"qwiklabs-gcp-00-2549be4858ea\"   name         = \"terraform\"   machine_type = \"e2-medium\"   zone         = \"us-east1-b\"</p> <p>boot_disk {     initialize_params {       image = \"debian-cloud/debian-11\"     }   }</p> <p>network_interface {     network = \"default\"     access_config {     }   } } terraform init terraform plan ``   # google_compute_instance.default will be created   + resource \"google_compute_instance\" \"default\" {       + can_ip_forward       = false       + cpu_platform         = (known after apply)       + deletion_protection  = false       + guest_accelerator    = (known after apply)       + id                   = (known after apply)       + instance_id          = (known after apply)       + label_fingerprint    = (known after apply)       + machine_type         = \"e2-medium\"       + metadata_fingerprint = (known after apply)       + name                 = \"terraform\"       + project              = \"qwiklabs-gcp-42390cc9da8a4c4b\"       + self_link            = (known after apply)       + tags_fingerprint     = (known after apply)       + zone                 = \"us-west1-c\"</p> <pre><code>  + boot_disk {\n      + auto_delete                = true\n      + device_name                = (known after apply)\n      + disk_encryption_key_sha256 = (known after apply)\n      + kms_key_self_link          = (known after apply)\n      + source                     = (known after apply)\n\n      + initialize_params {\n          + image  = \"debian-cloud/debian-11\"\n          + labels = (known after apply)\n          + size   = (known after apply)\n          + type   = (known after apply)\n        }\n    }\n\n  + network_interface {\n      + address            = (known after apply)\n      + name               = (known after apply)\n      + network            = \"default\"\n      + network_ip         = (known after apply)\n      + subnetwork         = (known after apply)\n      + subnetwork_project = (known after apply)\n\n      + access_config {\n          + assigned_nat_ip = (known after apply)\n          + nat_ip          = (known after apply)\n          + network_tier    = (known after apply)\n        }\n    }\n\n  + scheduling {\n      + automatic_restart   = (known after apply)\n      + on_host_maintenance = (known after apply)\n      + preemptible         = (known after apply)\n\n      + node_affinities {\n          + key      = (known after apply)\n          + operator = (known after apply)\n          + values   = (known after apply)\n        }\n    }\n\n}\n</code></pre> <p>Plan: 1 to add, 0 to change, 0 to destroy.</p> <p>Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve.</p> <p>Enter a value:  ``</p> <p>terraform apply</p> <p>Change something terraform apply</p> <p>terraform destroy</p>"},{"location":"blog/2024/08/03/managing-terraform-state-medium/","title":"Managing terraform state (Medium)","text":"<p>Note</p> <p>Test</p>"},{"location":"blog/2024/08/03/managing-terraform-state-medium/#overview","title":"Overview","text":"<p>Terraform must store the state about your managed infrastructure and configuration. This state is used by Terraform to map real-world resources to your configuration, keep track of metadata, and improve performance for large infrastructures.</p> <p>This state is stored by default in a local file named terraform.tfstate, but it can also be stored remotely, which works better in a team environment.</p> <p>Terraform uses this local state to create plans and make changes to your infrastructure. Before any operation, Terraform does a refresh to update the state with the real infrastructure.</p> <p>The primary purpose of Terraform state is to store bindings between objects in a remote system and resource instances declared in your configuration. When Terraform creates a remote object in response to a change of configuration, it will record the identity of that remote object against a particular resource instance and then potentially update or delete that object in response to future configuration changes.</p>"},{"location":"blog/2024/08/03/managing-terraform-state-medium/#task","title":"Task","text":"<p>Create a local backend. Create a Cloud Storage backend. Refresh your Terraform state. Import a Terraform configuration. Manage the imported configuration with Terraform.</p>"},{"location":"blog/2024/08/03/managing-terraform-state-medium/#_1","title":"Managing terraform state (Medium)","text":"<p>Purpose of Terraform state State is a necessary requirement for Terraform to function. People sometimes ask whether Terraform can work without state or not use state and just inspect cloud resources on every run. In the scenarios where Terraform may be able to get away without state, doing so would require shifting massive amounts of complexity from one place (state) to another place (the replacement concept). This section will help explain why Terraform state is required.</p> <p>Mapping to the real world Terraform requires some sort of database to map Terraform config to the real world. When your configuration contains a resource resource \"google_compute_instance\" \"foo\", Terraform uses this map to know that instance i-abcd1234 is represented by that resource.</p> <p>Terraform expects that each remote object is bound to only one resource instance, which is normally guaranteed because Terraform is responsible for creating the objects and recording their identities in the state. If you instead import objects that were created outside of Terraform, you must verify that each distinct object is imported to only one resource instance.</p> <p>If one remote object is bound to two or more resource instances, Terraform may take unexpected actions against those objects because the mapping from configuration to the remote object state has become ambiguous.</p> <p>Metadata In addition to tracking the mappings between resources and remote objects, Terraform must also track metadata such as resource dependencies.</p> <p>Terraform typically uses the configuration to determine dependency order. However, when you remove a resource from a Terraform configuration, Terraform must know how to delete that resource. Terraform can see that a mapping exists for a resource that is not in your configuration file and plan to destroy. However, because the resource no longer exists, the order cannot be determined from the configuration alone.</p> <p>To ensure correct operation, Terraform retains a copy of the most recent set of dependencies within the state. Now Terraform can still determine the correct order for destruction from the state when you delete one or more items from the configuration.</p> <p>This could be avoided if Terraform knew a required ordering between resource types. For example, Terraform could know that servers must be deleted before the subnets they are a part of. The complexity for this approach quickly becomes unmanageable, however: in addition to understanding the ordering semantics of every resource for every cloud, Terraform must also understand the ordering across providers.</p> <p>Terraform also stores other metadata for similar reasons, such as a pointer to the provider configuration that was most recently used with the resource in situations where multiple aliased providers are present.</p> <p>Performance In addition to basic mapping, Terraform stores a cache of the attribute values for all resources in the state. This is an optional feature of Terraform state and is used only as a performance improvement.</p> <p>When running a terraform plan, Terraform must know the current state of resources in order to effectively determine the changes needed to reach your desired configuration.</p> <p>For small infrastructures, Terraform can query your providers and sync the latest attributes from all your resources. This is the default behavior of Terraform: for every plan and apply, Terraform will sync all resources in your state.</p> <p>For larger infrastructures, querying every resource is too slow. Many cloud providers do not provide APIs to query multiple resources at the same time, and the round trip time for each resource is hundreds of milliseconds. In addition, cloud providers almost always have API rate limiting, so Terraform can only request a limited number of resources in a period of time. Larger users of Terraform frequently use both the -refresh=false flag and the -target flag in order to work around this. In these scenarios, the cached state is treated as the record of truth.</p> <p>Syncing In the default configuration, Terraform stores the state in a file in the current working directory where Terraform was run. This works when you are getting started, but when Terraform is used in a team, it is important for everyone to be working with the same state so that operations will be applied to the same remote objects.</p> <p>Remote state is the recommended solution to this problem. With a fully featured state backend, Terraform can use remote locking as a measure to avoid multiple different users accidentally running Terraform at the same time; this ensures that each Terraform run begins with the most recent updated state.</p> <p>State locking If supported by your backend, Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state.</p> <p>State locking happens automatically on all operations that could write state. You won't see any message that it is happening. If state locking fails, Terraform will not continue. You can disable state locking for most commands with the -lock flag, but it is not recommended.</p> <p>If acquiring the lock is taking longer than expected, Terraform will output a status message. If Terraform doesn't output a message, state locking is still occurring.</p> <p>Not all backends support locking. View the list of backend types for details on whether a backend supports locking.</p> <p>Workspaces Each Terraform configuration has an associated backend that defines how operations are executed and where persistent data such as the Terraform state is stored.</p> <p>The persistent data stored in the backend belongs to a workspace. Initially the backend has only one workspace, called default, and thus only one Terraform state is associated with that configuration.</p> <p>Certain backends support multiple named workspaces, which allows multiple states to be associated with a single configuration. The configuration still has only one backend, but multiple distinct instances of that configuration can be deployed without configuring a new backend or changing authentication credentials</p>"},{"location":"blog/2024/08/03/managing-terraform-state-medium/#work-with-backend","title":"Work with backend","text":"<p>A backend in Terraform determines how state is loaded and how an operation such as apply is executed. This abstraction enables non-local file state storage, remote execution, etc.</p> <p>touch main.tf provider \"google\" {   project     = \"qwiklabs-gcp-00-dbfaa858d882\"   region      = \"europe-west4\" }</p> <p>resource \"google_storage_bucket\" \"test-bucket-for-state\" {   name        = \"qwiklabs-gcp-00-dbfaa858d882\"   location    = \"US\"   uniform_bucket_level_access = true } terraform {   backend \"local\" {     path = \"terraform/state/terraform.tfstate\"   } } terraform apply terraform show</p> <p>Add a Cloud Storage backend A Cloud Storage backend stores the state as an object in a configurable prefix in a given bucket on Cloud Storage. This backend also supports state locking. This will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state.</p> <p>State locking happens automatically on all operations that could write state. You won't see any message that it is happening. If state locking fails, Terraform will not continue. You can disable state locking for most commands with the -lock flag, but this is not recommended.</p> <p>replace in main.tf terraform {   backend \"gcs\" {     bucket  = \"qwiklabs-gcp-00-dbfaa858d882\"     prefix  = \"terraform/state\"   } } Initialize your backend again, this time to automatically migrate the state: terraform init -migrate-state The state file will be exitst on Cloud storage/Buckets</p> <p>Clean up workspace - change backto local backend - In the main.tf file, add the force_destroy = true argument to your google_storage_bucket resource. When you delete a bucket, this boolean option will delete all contained objects. If you try to delete a bucket that contains objects, Terraform will fail that run.</p> <p>resource \"google_storage_bucket\" \"test-bucket-for-state\" {   name        = \"qwiklabs-gcp-03-c26136e27648\"   location    = \"US\"   uniform_bucket_level_access = true   force_destroy = true }</p>"},{"location":"blog/2024/08/03/managing-terraform-state-medium/#task-2-import-terraform-configuration","title":"Task 2. Import Terraform configuration","text":"<p>In this section, you will import an existing Docker container and image into an empty Terraform workspace. By doing so, you will learn strategies and considerations for importing real-world infrastructure into Terraform.</p> <ul> <li> <p>The default Terraform workflow involves creating and managing infrastructure entirely with Terraform.</p> </li> <li> <p>Write a Terraform configuration that defines the infrastructure you want to create.</p> </li> <li> <p>Review the Terraform plan to ensure that the configuration will result in the expected state and infrastructure.</p> </li> <li> <p>Apply the configuration to create your Terraform state and infrastructure.</p> </li> </ul>"},{"location":"blog/2024/08/03/managing-terraform-state-medium/#create-docker-container","title":"Create docker container","text":"<p>docker run --name hashicorp-learn --detach --publish 8080:80 nginx:latest</p> <p></p>"},{"location":"posts/Terraform_01/","title":"Terraform basic lab 01","text":"<p>touch instance.tf</p> <p>resource \"google_compute_instance\" \"terraform\" {   project      = \"qwiklabs-gcp-00-2549be4858ea\"   name         = \"terraform\"   machine_type = \"e2-medium\"   zone         = \"us-east1-b\"</p> <p>boot_disk {     initialize_params {       image = \"debian-cloud/debian-11\"     }   }</p> <p>network_interface {     network = \"default\"     access_config {     }   } } terraform init terraform plan ``</p> <p># google_compute_instance.default will be created   + resource \"google_compute_instance\" \"default\" {       + can_ip_forward       = false       + cpu_platform         = (known after apply)       + deletion_protection  = false       + guest_accelerator    = (known after apply)       + id                   = (known after apply)       + instance_id          = (known after apply)       + label_fingerprint    = (known after apply)       + machine_type         = \"e2-medium\"       + metadata_fingerprint = (known after apply)       + name                 = \"terraform\"       + project              = \"qwiklabs-gcp-42390cc9da8a4c4b\"       + self_link            = (known after apply)       + tags_fingerprint     = (known after apply)       + zone                 = \"us-west1-c\"</p> <pre><code>  + boot_disk {\n      + auto_delete                = true\n      + device_name                = (known after apply)\n      + disk_encryption_key_sha256 = (known after apply)\n      + kms_key_self_link          = (known after apply)\n      + source                     = (known after apply)\n\n      + initialize_params {\n          + image  = \"debian-cloud/debian-11\"\n          + labels = (known after apply)\n          + size   = (known after apply)\n          + type   = (known after apply)\n        }\n    }\n\n  + network_interface {\n      + address            = (known after apply)\n      + name               = (known after apply)\n      + network            = \"default\"\n      + network_ip         = (known after apply)\n      + subnetwork         = (known after apply)\n      + subnetwork_project = (known after apply)\n\n      + access_config {\n          + assigned_nat_ip = (known after apply)\n          + nat_ip          = (known after apply)\n          + network_tier    = (known after apply)\n        }\n    }\n\n  + scheduling {\n      + automatic_restart   = (known after apply)\n      + on_host_maintenance = (known after apply)\n      + preemptible         = (known after apply)\n\n      + node_affinities {\n          + key      = (known after apply)\n          + operator = (known after apply)\n          + values   = (known after apply)\n        }\n    }\n\n}\n</code></pre> <p>Plan: 1 to add, 0 to change, 0 to destroy.</p> <p>Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve.</p> <p>Enter a value:  ``</p> <p>terraform apply</p> <p>Change something terraform apply</p> <p>terraform destroy</p>"},{"location":"posts/Terraform_02/","title":"Managing terraform state (Medium)","text":""},{"location":"posts/Terraform_02/#overview","title":"Overview","text":"<p>Terraform must store the state about your managed infrastructure and configuration. This state is used by Terraform to map real-world resources to your configuration, keep track of metadata, and improve performance for large infrastructures.</p> <p>This state is stored by default in a local file named terraform.tfstate, but it can also be stored remotely, which works better in a team environment.</p> <p>Terraform uses this local state to create plans and make changes to your infrastructure. Before any operation, Terraform does a refresh to update the state with the real infrastructure.</p> <p>The primary purpose of Terraform state is to store bindings between objects in a remote system and resource instances declared in your configuration. When Terraform creates a remote object in response to a change of configuration, it will record the identity of that remote object against a particular resource instance and then potentially update or delete that object in response to future configuration changes.</p>"},{"location":"posts/Terraform_02/#task","title":"Task","text":"<p>Create a local backend. Create a Cloud Storage backend. Refresh your Terraform state. Import a Terraform configuration. Manage the imported configuration with Terraform.</p>"},{"location":"posts/Terraform_02/#_1","title":"Managing terraform state (Medium)","text":"<p>Purpose of Terraform state State is a necessary requirement for Terraform to function. People sometimes ask whether Terraform can work without state or not use state and just inspect cloud resources on every run. In the scenarios where Terraform may be able to get away without state, doing so would require shifting massive amounts of complexity from one place (state) to another place (the replacement concept). This section will help explain why Terraform state is required.</p> <p>Mapping to the real world Terraform requires some sort of database to map Terraform config to the real world. When your configuration contains a resource resource \"google_compute_instance\" \"foo\", Terraform uses this map to know that instance i-abcd1234 is represented by that resource.</p> <p>Terraform expects that each remote object is bound to only one resource instance, which is normally guaranteed because Terraform is responsible for creating the objects and recording their identities in the state. If you instead import objects that were created outside of Terraform, you must verify that each distinct object is imported to only one resource instance.</p> <p>If one remote object is bound to two or more resource instances, Terraform may take unexpected actions against those objects because the mapping from configuration to the remote object state has become ambiguous.</p> <p>Metadata In addition to tracking the mappings between resources and remote objects, Terraform must also track metadata such as resource dependencies.</p> <p>Terraform typically uses the configuration to determine dependency order. However, when you remove a resource from a Terraform configuration, Terraform must know how to delete that resource. Terraform can see that a mapping exists for a resource that is not in your configuration file and plan to destroy. However, because the resource no longer exists, the order cannot be determined from the configuration alone.</p> <p>To ensure correct operation, Terraform retains a copy of the most recent set of dependencies within the state. Now Terraform can still determine the correct order for destruction from the state when you delete one or more items from the configuration.</p> <p>This could be avoided if Terraform knew a required ordering between resource types. For example, Terraform could know that servers must be deleted before the subnets they are a part of. The complexity for this approach quickly becomes unmanageable, however: in addition to understanding the ordering semantics of every resource for every cloud, Terraform must also understand the ordering across providers.</p> <p>Terraform also stores other metadata for similar reasons, such as a pointer to the provider configuration that was most recently used with the resource in situations where multiple aliased providers are present.</p> <p>Performance In addition to basic mapping, Terraform stores a cache of the attribute values for all resources in the state. This is an optional feature of Terraform state and is used only as a performance improvement.</p> <p>When running a terraform plan, Terraform must know the current state of resources in order to effectively determine the changes needed to reach your desired configuration.</p> <p>For small infrastructures, Terraform can query your providers and sync the latest attributes from all your resources. This is the default behavior of Terraform: for every plan and apply, Terraform will sync all resources in your state.</p> <p>For larger infrastructures, querying every resource is too slow. Many cloud providers do not provide APIs to query multiple resources at the same time, and the round trip time for each resource is hundreds of milliseconds. In addition, cloud providers almost always have API rate limiting, so Terraform can only request a limited number of resources in a period of time. Larger users of Terraform frequently use both the -refresh=false flag and the -target flag in order to work around this. In these scenarios, the cached state is treated as the record of truth.</p> <p>Syncing In the default configuration, Terraform stores the state in a file in the current working directory where Terraform was run. This works when you are getting started, but when Terraform is used in a team, it is important for everyone to be working with the same state so that operations will be applied to the same remote objects.</p> <p>Remote state is the recommended solution to this problem. With a fully featured state backend, Terraform can use remote locking as a measure to avoid multiple different users accidentally running Terraform at the same time; this ensures that each Terraform run begins with the most recent updated state.</p> <p>State locking If supported by your backend, Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state.</p> <p>State locking happens automatically on all operations that could write state. You won't see any message that it is happening. If state locking fails, Terraform will not continue. You can disable state locking for most commands with the -lock flag, but it is not recommended.</p> <p>If acquiring the lock is taking longer than expected, Terraform will output a status message. If Terraform doesn't output a message, state locking is still occurring.</p> <p>Not all backends support locking. View the list of backend types for details on whether a backend supports locking.</p> <p>Workspaces Each Terraform configuration has an associated backend that defines how operations are executed and where persistent data such as the Terraform state is stored.</p> <p>The persistent data stored in the backend belongs to a workspace. Initially the backend has only one workspace, called default, and thus only one Terraform state is associated with that configuration.</p> <p>Certain backends support multiple named workspaces, which allows multiple states to be associated with a single configuration. The configuration still has only one backend, but multiple distinct instances of that configuration can be deployed without configuring a new backend or changing authentication credentials</p>"},{"location":"posts/Terraform_02/#work-with-backend","title":"Work with backend","text":"<p>A backend in Terraform determines how state is loaded and how an operation such as apply is executed. This abstraction enables non-local file state storage, remote execution, etc.</p> <p>touch main.tf provider \"google\" {   project     = \"qwiklabs-gcp-00-dbfaa858d882\"   region      = \"europe-west4\" }</p> <p>resource \"google_storage_bucket\" \"test-bucket-for-state\" {   name        = \"qwiklabs-gcp-00-dbfaa858d882\"   location    = \"US\"   uniform_bucket_level_access = true } terraform {   backend \"local\" {     path = \"terraform/state/terraform.tfstate\"   } } terraform apply terraform show</p> <p>Add a Cloud Storage backend A Cloud Storage backend stores the state as an object in a configurable prefix in a given bucket on Cloud Storage. This backend also supports state locking. This will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state.</p> <p>State locking happens automatically on all operations that could write state. You won't see any message that it is happening. If state locking fails, Terraform will not continue. You can disable state locking for most commands with the -lock flag, but this is not recommended.</p> <p>replace in main.tf terraform {   backend \"gcs\" {     bucket  = \"qwiklabs-gcp-00-dbfaa858d882\"     prefix  = \"terraform/state\"   } } Initialize your backend again, this time to automatically migrate the state: terraform init -migrate-state The state file will be exitst on Cloud storage/Buckets</p> <p>Clean up workspace - change backto local backend - In the main.tf file, add the force_destroy = true argument to your google_storage_bucket resource. When you delete a bucket, this boolean option will delete all contained objects. If you try to delete a bucket that contains objects, Terraform will fail that run.</p> <p>resource \"google_storage_bucket\" \"test-bucket-for-state\" {   name        = \"qwiklabs-gcp-03-c26136e27648\"   location    = \"US\"   uniform_bucket_level_access = true   force_destroy = true }</p>"},{"location":"posts/Terraform_02/#task-2-import-terraform-configuration","title":"Task 2. Import Terraform configuration","text":"<p>In this section, you will import an existing Docker container and image into an empty Terraform workspace. By doing so, you will learn strategies and considerations for importing real-world infrastructure into Terraform.</p> <ul> <li> <p>The default Terraform workflow involves creating and managing infrastructure entirely with Terraform.</p> </li> <li> <p>Write a Terraform configuration that defines the infrastructure you want to create.</p> </li> <li> <p>Review the Terraform plan to ensure that the configuration will result in the expected state and infrastructure.</p> </li> <li> <p>Apply the configuration to create your Terraform state and infrastructure.</p> </li> </ul>"},{"location":"posts/Terraform_02/#create-docker-container","title":"Create docker container","text":"<p>docker run --name hashicorp-learn --detach --publish 8080:80 nginx:latest</p> <p></p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/kubernetes/","title":"Kubernetes","text":""},{"location":"blog/category/study/","title":"Study","text":""},{"location":"blog/category/terraform/","title":"Terraform","text":""}]}